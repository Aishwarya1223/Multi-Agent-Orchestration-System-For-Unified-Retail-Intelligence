<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>OmniFlow â€“ Multimodal Agent</title>

  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 900px;
      margin: auto;
      padding: 20px;
      background: #f5f5f5;
    }

    h2 {
      text-align: center;
    }

    .input-group {
      display: flex;
      gap: 10px;
      margin: 20px 0;
    }

    input {
      flex: 1;
      padding: 10px;
      font-size: 16px;
    }

    button {
      padding: 10px 14px;
      border-radius: 4px;
      border: none;
      cursor: pointer;
    }

    .btn-primary { background: #0d6efd; color: white; }
    .btn-whisper { background: #6f42c1; color: white; }
    .btn-danger { background: #dc3545; color: white; }
    .btn-camera { background: #fd7e14; color: white; }

    .card {
      background: white;
      padding: 20px;
      border-radius: 8px;
      margin-top: 20px;
    }

    pre {
      background: #f8f9fa;
      padding: 15px;
      min-height: 60px;
      border-radius: 4px;
      white-space: pre-wrap;
    }

    video, img {
      border: 1px solid #ddd;
      border-radius: 4px;
    }

    .voice-indicator {
      display: inline-block;
      width: 12px;
      height: 12px;
      border-radius: 50%;
      margin-left: 5px;
      background: #dc3545;
    }

    .voice-indicator.listening {
      background: #28a745;
      animation: pulse 1.5s infinite;
    }

    .voice-indicator.processing {
      background: #ffc107;
      animation: pulse 1s infinite;
    }

    .voice-indicator.speaking {
      background: #007bff;
      animation: pulse 0.8s infinite;
    }

    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }

    .transcript-box {
      background: #e9ecef;
      border: 1px solid #dee2e6;
      border-radius: 4px;
      padding: 10px;
      margin: 10px 0;
      font-family: monospace;
      font-size: 14px;
      max-height: 150px;
      overflow-y: auto;
    }
  </style>
</head>
<body>

<h2>OmniFlow â€“ Multimodal Agent</h2>

<!-- ================= INPUT ================= -->
<div class="input-group">
  <input id="query" placeholder="Ask something..." />
  <button class="btn-primary" onclick="sendTextWithImage()">ðŸ“¤ Send</button>
  <button class="btn-whisper" id="whisperBtn" onclick="toggleWhisper()">
    ðŸŽ¤ Voice Call
    <span class="voice-indicator" id="voiceIndicator"></span>
  </button>
</div>

<!-- ================= RESPONSE ================= -->
<div class="card">
  <h3>Response</h3>
  <pre id="responseBox">Idleâ€¦</pre>
  
  <div class="transcript-box" id="transcriptBox" style="display: none;">
    <strong>Live Transcript:</strong>
    <div id="liveTranscript"></div>
  </div>
</div>

<!-- ================= CAMERA (UNCHANGED LOGIC) ================= -->
<div class="card">
  <h3>Camera</h3>
  <video id="video" autoplay playsinline width="300"></video>
  <p id="cameraStatus">Initializing cameraâ€¦</p>

  <button class="btn-camera" onclick="captureImage()">ðŸ“¸ Capture</button>
  <button class="btn-danger" onclick="clearImage()">ðŸ—‘ Clear</button>

  <div id="capturedImageContainer" style="display:none;">
    <h4>Captured Image</h4>
    <img id="capturedImage" width="300" />
    <canvas id="canvas" style="display:none;"></canvas>
  </div>
</div>

<script>
/* ======================================================
   CAMERA (UNCHANGED BEHAVIOR)
====================================================== */

let capturedImageData = null;

async function startCamera() {
  const video = document.getElementById("video");
  const status = document.getElementById("cameraStatus");
  
  try {
    if (location.protocol !== "https:" && location.hostname !== "localhost") {
      throw new Error("HTTPS required");
    }

    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;
    status.innerText = "Camera active";
    status.style.color = "green";
  } catch (err) {
    status.innerText = "Camera unavailable";
    status.style.color = "red";
  }
}

window.addEventListener("load", startCamera);

function captureImage() {
  const video = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const img = document.getElementById("capturedImage");

  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
  canvas.getContext("2d").drawImage(video, 0, 0);

  capturedImageData = canvas.toDataURL("image/jpeg", 0.8);
  img.src = capturedImageData;
  document.getElementById("capturedImageContainer").style.display = "block";
}

function clearImage() {
  capturedImageData = null;
  document.getElementById("capturedImageContainer").style.display = "none";
}

/* ======================================================
   TEXT + IMAGE QUERY
====================================================== */

async function sendTextWithImage() {
  const query = document.getElementById("query").value;
  const responseBox = document.getElementById("responseBox");

  responseBox.innerText = "Processingâ€¦";

  try {
    const res = await fetch("/api/query/", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        query: query || "Describe the image",
        user_email: "test@omni.com",
        image: capturedImageData
      })
    });

    const data = await res.json();
    responseBox.innerText = data?.response?.answer || "No response";

    // Speak the response using browser speech synthesis
    if (data?.response?.answer) {
      const speech = new SpeechSynthesisUtterance(data.response.answer);
      speech.rate = 0.9;
      speech.pitch = 1;
      window.speechSynthesis.speak(speech);
    }

  } catch (err) {
    responseBox.innerText = "Server error";
  }
}

/* ======================================================
   GPT WHISPER VOICE IMPLEMENTATION
====================================================== */

let whisperActive = false;
let mediaRecorder = null;
let audioChunks = [];
let recognition = null;
let isProcessing = false;

// Initialize Web Speech API for fallback
if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  recognition = new SpeechRecognition();
  recognition.continuous = true;
  recognition.interimResults = true;
  recognition.lang = 'en-US';

  recognition.onresult = (event) => {
    if (!whisperActive || isProcessing) return;
    
    const current = event.resultIndex;
    const transcript = event.results[current][0].transcript;
    
    if (event.results[current].isFinal) {
      addTranscript(transcript, 'user');
      processVoiceInput(transcript);
    }
  };

  recognition.onerror = (event) => {
    console.error('Speech recognition error:', event.error);
    updateStatus('Microphone error: ' + event.error, 'error');
  };

  recognition.onend = () => {
    if (whisperActive && !isProcessing) {
      // Restart recognition if still active
      setTimeout(() => {
        if (whisperActive) {
          recognition.start();
        }
      }, 100);
    }
  };
}

async function toggleWhisper() {
  whisperActive ? stopWhisper() : startWhisper();
}

async function startWhisper() {
  const responseBox = document.getElementById("responseBox");
  const btn = document.getElementById("whisperBtn");
  const indicator = document.getElementById("voiceIndicator");
  const transcriptBox = document.getElementById("transcriptBox");
  const liveTranscript = document.getElementById("liveTranscript");

  if (whisperActive) return;

  try {
    // Request microphone access
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        sampleRate: 16000
      } 
    });

    // Update UI
    whisperActive = true;
    btn.textContent = "ðŸ”´ End Call";
    btn.classList.remove("btn-whisper");
    btn.classList.add("btn-danger");
    indicator.classList.add("listening");
    transcriptBox.style.display = "block";
    liveTranscript.innerHTML = "";
    responseBox.innerText = "ðŸŽ¤ Listening... Speak naturally!";
    addTranscript("Call started - Sarah is listening", "system");

    // Start speech recognition
    if (recognition) {
      recognition.start();
    }

    // Start recording for Whisper processing
    mediaRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm;codecs=opus'
    });

    audioChunks = [];
    
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data);
      }
    };

    mediaRecorder.onstop = () => {
      const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
      processAudioWithWhisper(audioBlob);
      audioChunks = [];
    };

    // Start recording in chunks
    mediaRecorder.start(1000); // Record 1-second chunks

    // Set up continuous recording
    setInterval(() => {
      if (whisperActive && mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
        mediaRecorder.start();
      }
    }, 2000);

  } catch (err) {
    console.error('Error starting voice:', err);
    updateStatus('Microphone access denied', 'error');
  }
}

function stopWhisper() {
  const responseBox = document.getElementById("responseBox");
  const btn = document.getElementById("whisperBtn");
  const indicator = document.getElementById("voiceIndicator");

  if (!whisperActive) return;

  whisperActive = false;
  btn.textContent = "ðŸŽ¤ Voice Call";
  btn.classList.remove("btn-danger");
  btn.classList.add("btn-whisper");
  indicator.classList.remove("listening", "processing", "speaking");

  // Stop speech recognition
  if (recognition) {
    recognition.stop();
  }

  // Stop recording
  if (mediaRecorder && mediaRecorder.state === 'recording') {
    mediaRecorder.stop();
  }

  responseBox.innerText = "ðŸ“ž Call ended";
  addTranscript("Call ended", "system");
}

async function processAudioWithWhisper(audioBlob) {
  if (isProcessing) return;
  
  isProcessing = true;
  updateStatus("Processing speech...", "processing");

  try {
    // Create FormData for file upload
    const formData = new FormData();
    formData.append('audio', audioBlob, 'recording.webm');
    formData.append('model', 'whisper-1');

    // Send to Whisper API endpoint
    const response = await fetch('/api/whisper/transcribe/', {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error('Transcription failed');
    }

    const data = await response.json();
    
    if (data.transcript) {
      addTranscript(data.transcript, 'user');
      await processVoiceInput(data.transcript);
    }

  } catch (err) {
    console.error('Whisper processing error:', err);
    // Fallback to speech recognition if available
    if (recognition) {
      updateStatus("Using speech recognition fallback", "normal");
    } else {
      updateStatus("Speech processing failed", "error");
    }
  } finally {
    isProcessing = false;
    updateStatus("Listening...", "normal");
  }
}

async function processVoiceInput(transcript) {
  const responseBox = document.getElementById("responseBox");
  
  try {
    updateStatus("Thinking...", "processing");
    
    // Send transcript to your backend API
    const response = await fetch("/api/query/", {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        query: transcript,
        user_email: "voice@omni.com",
        mode: "voice"
      })
    });

    const data = await response.json();
    
    if (data.response && data.response.answer) {
      // Display Sarah's response
      responseBox.innerText = `ðŸ¤– Sarah: ${data.response.answer}`;
      addTranscript(data.response.answer, 'sarah');
      
      // Speak the response using browser speech synthesis (like Vapi would)
      speakResponse(data.response.answer);
      
      updateStatus("Listening...", "normal");
    } else {
      responseBox.innerText = "Sorry, I didn't understand that.";
      updateStatus("Listening...", "normal");
    }

  } catch (err) {
    console.error('Voice processing error:', err);
    responseBox.innerText = "Error processing voice input";
    updateStatus("Error", "error");
  }
}

function speakResponse(text) {
  // Use browser's built-in speech synthesis (like Vapi's voice)
  const speech = new SpeechSynthesisUtterance(text);
  
  // Configure voice to sound more natural (like Sarah's voice)
  const voices = window.speechSynthesis.getVoices();
  
  // Try to find a female voice (like Sarah)
  const femaleVoice = voices.find(voice => 
    voice.name.includes('Female') || 
    voice.name.includes('Samantha') || 
    voice.name.includes('Karen') ||
    voice.name.includes('Google US English')
  ) || voices[0]; // fallback to first voice
  
  speech.voice = femaleVoice;
  speech.rate = 0.9; // Slightly slower for more natural speech
  speech.pitch = 1.0;
  speech.volume = 1.0;
  
  // Add pauses for more natural conversation flow
  speech.onend = () => {
    if (whisperActive) {
      updateStatus("Listening...", "normal");
    }
  };

  window.speechSynthesis.speak(speech);
}

function addTranscript(text, speaker) {
  const liveTranscript = document.getElementById("liveTranscript");
  const timestamp = new Date().toLocaleTimeString();
  
  const entry = document.createElement('div');
  entry.style.marginBottom = '5px';
  
  if (speaker === 'user') {
    entry.innerHTML = `<strong>[${timestamp}] You:</strong> ${text}`;
    entry.style.color = '#007bff';
  } else if (speaker === 'sarah') {
    entry.innerHTML = `<strong>[${timestamp}] Sarah:</strong> ${text}`;
    entry.style.color = '#28a745';
  } else {
    entry.innerHTML = `<strong>[${timestamp}] System:</strong> ${text}`;
    entry.style.color = '#6c757d';
  }
  
  liveTranscript.appendChild(entry);
  liveTranscript.scrollTop = liveTranscript.scrollHeight;
}

function updateStatus(message, type) {
  const responseBox = document.getElementById("responseBox");
  const indicator = document.getElementById("voiceIndicator");
  
  if (!whisperActive) return;
  
  responseBox.innerText = message;
  
  // Update indicator based on status
  indicator.classList.remove("listening", "processing", "speaking");
  
  if (type === 'listening') {
    indicator.classList.add("listening");
  } else if (type === 'processing') {
    indicator.classList.add("processing");
  } else if (type === 'speaking') {
    indicator.classList.add("speaking");
  }
}

// Error handling for microphone permissions
navigator.mediaDevices.getUserMedia = navigator.mediaDevices.getUserMedia || 
  navigator.mediaDevices.webkitGetUserMedia || 
  navigator.mediaDevices.mozGetUserMedia;

// Auto-restart speech recognition if it stops unexpectedly
if (recognition) {
  recognition.onstart = () => {
    console.log('Speech recognition started');
  };
}
</script>

</body>
</html>
